# -*- coding: utf-8 -*-
"""VERAANTHONIO._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12UBmVlIVJzrxXofrYoaYckWfDp6hMTuo
"""

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn
from sklearn.model_selection import train_test_split

# Load the CSV file into a pandas DataFrame
df = pd.read_csv('male_players (legacy).csv', na_values='-')

# Display basic information about the DataFrame
df.info()

# Check for missing values and drop columns with too many missing values
missing_values = df.isnull().sum()
print(missing_values)

df.drop(['player_id', 'player_url', 'short_name', 'long_name', 'player_face_url',
    'fifa_version', 'fifa_update', 'fifa_update_date',
    'league_id', 'league_name', 'league_level',
    'club_team_id', 'club_name', 'club_position', 'club_jersey_number',
    'club_loaned_from', 'club_joined_date', 'club_contract_valid_until_year',
    'nationality_id', 'nationality_name', 'nation_team_id',
    'nation_position', 'nation_jersey_number', 'player_tags', 'player_traits'],axis=1, inplace=True)

L=[]
L_less=[]
for i in df.columns:
    if((df[i].isnull().sum())<(0.3*(df.shape[0]))):
        L.append(i)
    else:
        L_less.append(i)

df=df[L]

df.head()

# Separate columns by data type
numerical_columns = df.select_dtypes(include=np.number)
categorical_columns = df.select_dtypes(include=['object'])

# Imputation of missing values
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp= IterativeImputer(max_iter=10, random_state=0)
numerical_columns=pd.DataFrame(np.round(imp.fit_transform(numerical_columns)),columns=numerical_columns.columns) #all colums without missing values

categorical_columns = pd.get_dummies(categorical_columns).astype(int)

categorical_columns

# Summary statistics
df.describe()

plt.figure(figsize=(10, 6))
plt.hist(df['overall'], bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Overall Ratings')
plt.xlabel('Overall Rating')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# Compute the correlation matrix for the numeric columns
corr_matrix = numerical_columns.corr()
print(corr_matrix)

# Creating a new feature 'passing_dribbling_ratio'
numerical_columns['passing_dribbling_ratio'] = numerical_columns['passing'] /numerical_columns['dribbling']

# Creating an interaction term 'shooting_aggression_interaction'
numerical_columns['shooting_aggression_interaction'] =numerical_columns['shooting'] * numerical_columns['mentality_aggression']

corr_matrix["overall"].sort_values(ascending=False)

#feature importance to select most relevant features from categorical_columns

# Set a threshold for the absolute value of the correlation coefficient
threshold = 0.1

# Identify columns that have a low correlation with the target variable
unnecessary_columns = corr_matrix["overall"].loc[lambda x: abs(x) < threshold].index.tolist()

# Drop the unnecessary columns from the DataFrame
numerical_columns.drop(unnecessary_columns, axis=1, inplace=True)

# Combine numerical and categorical columns
X = pd.concat([numerical_columns, categorical_columns], axis=1)

# Separate features and target
y=numerical_columns['overall']
X = X.drop('overall', axis=1)


X

categorical_columns

y,actual_values=pd.factorize(y)
actual_values

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,VotingRegressor
import xgboost as xgb

Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.2,random_state=42)

import pickle as pkl



# Define the models
models = [
    RandomForestRegressor(random_state=42),
    xgb.XGBRegressor(random_state=42),
    GradientBoostingRegressor(random_state=42)
]

# Train, evaluate, and save each model
model_scores = {}
for model in models:
    # Evaluate using cross-validation
    scores = cross_val_score(model, Xtrain, Ytrain, cv=5)
    mean_score = scores.mean()

    # Store the mean cross-validation score
    model_scores[model.__class__.__name__] = mean_score

    # Train the model on the full training set
    model.fit(Xtrain, Ytrain)

    # Save the trained model
    pkl.dump(model, open(f'C:\\Users\\vera.anthonio\\{model.__class__.__name__}.pkl', 'wb'))


    # Print the cross-validation scores
    print(f"{model.__class__.__name__} Cross-Validation Scores: {scores}")
    print(f"Mean {model.__class__.__name__} Cross-Validation Score: {mean_score}")

# Create and evaluate the ensemble model
ensemble_model = VotingRegressor([
    ('rf', models[0]),
    ('xgb', models[1]),
    ('gb', models[2])
])

# Evaluate the ensemble model using cross-validation
ensemble_scores = cross_val_score(ensemble_model, Xtrain, Ytrain, cv=5)
ensemble_mean_score = ensemble_scores.mean()

# Store the mean cross-validation score of the ensemble model
model_scores['Ensemble'] = ensemble_mean_score

# Train the ensemble model on the full training set
ensemble_model.fit(Xtrain, Ytrain)

# Save the ensemble model
pkl.dump(ensemble_model, open('C:\\Users\\vera.anthonio\\Ensemble.pkl', 'wb'))

# Print the ensemble model's cross-validation scores
print(f"Ensemble Model Cross-Validation Scores: {ensemble_scores}")
print(f"Mean Ensemble Model Cross-Validation Score: {ensemble_mean_score}")

# Compare the models and display the best model
best_model_name = max(model_scores, key=model_scores.get)
best_model_score = model_scores[best_model_name]

print(f"Best Model: {best_model_name} with Mean Cross-Validation Score: {best_model_score}")

# Define the models with parameter grids for tuning
param_grids = {
    'rf': {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'xgb': {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 6, 9],
        'learning_rate': [0.01, 0.1, 0.3]
    },
    'gb': {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 6, 9],
        'learning_rate': [0.01, 0.1, 0.3]
    }
}

# Fine-tune the models using GridSearchCV
def fine_tune_model(model, param_grid, X_train, y_train):
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error')
    grid_search.fit(Xtrain, Ytrain)
    print(f"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}")
    print(f"Best MAE score for {model.__class__.__name__}: {-grid_search.best_score_}")
    return grid_search.best_estimator_

# Define the models
rf_model = RandomForestRegressor(random_state=42)
xgb_model = xgb.XGBRegressor(random_state=42)
gb_model = GradientBoostingRegressor(random_state=42)

# Fine-tune each model
best_rf_model = fine_tune_model(rf_model, param_grids['rf'], Xtrain, Ytrain)

best_xgb_model = fine_tune_model(xgb_model, param_grids['xgb'], Xtrain, Ytrain)

best_gb_model = fine_tune_model(gb_model, param_grids['gb'], Xtrain, Ytrain)

# Create the ensemble model with the fine-tuned base models
ensemble_model = VotingRegressor([
    ('rf', best_rf_model),
    ('xgb', best_xgb_model),
    ('gb', best_gb_model)
])

# Evaluate the ensemble model using cross-validation
ensemble_scores = cross_val_score(ensemble_model, Xtrain, Ytrain, cv=5)
ensemble_mean_score = ensemble_scores.mean()

# Store the mean cross-validation score of the ensemble model
model_scores = {'Ensemble': ensemble_mean_score}

# Train the ensemble model on the full training set
ensemble_model.fit(Xtrain, Ytrain)

# Save the ensemble model
pkl.dump(ensemble_model, open('C:\\Users\\vera.anthonio\\Ensemble.pkl', 'wb'))

# Print the ensemble model's cross-validation scores
print(f"Ensemble Model Cross-Validation Scores: {ensemble_scores}")
print(f"Mean Ensemble Model Cross-Validation Score: {ensemble_mean_score}")

# Evaluate the ensemble model on the test set
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

y_pred = ensemble_model.predict(Xtest)
test_mae = mean_absolute_error(Ytest, y_pred)
test_mse = mean_squared_error(Ytest, y_pred)
test_r2 = r2_score(Ytest, y_pred)

print(f"Ensemble Model Test MAE: {test_mae}")
print(f"Ensemble Model Test MSE: {test_mse}")
print(f"Ensemble Model Test RÂ²: {test_r2}")

# Compare the models and display the best model
best_model_name = max(model_scores, key=model_scores.get)
best_model_score = model_scores[best_model_name]

print(f"Best Model: {best_model_name} with Mean Cross-Validation Score: {best_model_score}")

import pandas as pd
import numpy as np
def preprocess_data(data_path):
  """
  This function automates the preprocessing steps performed on the male_players dataset.

  Args:
    data_path (str): The path to the CSV file containing the data.

  Returns:
    pd.DataFrame: The preprocessed data as a pandas DataFrame.
  """

  # Load the data from the CSV file
  df = pd.read_csv(data_path, na_values='-')

  # Drop unnecessary columns
  columns_to_drop = ['player_id', 'player_url', 'short_name', 'long_name', 'player_face_url',
                   'fifa_version', 'fifa_update', 'fifa_update_date',
                   'league_id', 'league_name', 'league_level',
                   'club_team_id', 'club_name', 'club_position', 'club_jersey_number',
                   'club_loaned_from', 'club_joined_date', 'club_contract_valid_until_year',
                   'nationality_id', 'nationality_name', 'nation_team_id',
                   'nation_position', 'nation_jersey_number', 'player_tags', 'player_traits']
  # Filter columns that exist in the DataFrame
  columns_to_drop = [col for col in columns_to_drop if col in df.columns]

  df.drop(columns_to_drop, axis=1, inplace=True, errors='ignore')

  # Drop columns with more than 30% missing values
  missing_values = df.isnull().sum()
  columns_to_drop = missing_values[missing_values > df.shape[0] * 0.3].index
  df.drop(columns_to_drop, axis=1, inplace=True)

  # Separate columns by data type
  numerical_columns = df.select_dtypes(include=np.number)
  categorical_columns = df.select_dtypes(include=['object'])

  # Impute missing values
  imp = IterativeImputer(max_iter=10, random_state=0)
  numerical_columns = pd.DataFrame(np.round(imp.fit_transform(numerical_columns)), columns=numerical_columns.columns)

  # Convert categorical columns to one-hot encoding
  categorical_columns = pd.get_dummies(categorical_columns).astype(int)

  # Create new features
  numerical_columns['passing_dribbling_ratio'] = numerical_columns['passing'] / numerical_columns['dribbling']
  numerical_columns['shooting_aggression_interaction'] = numerical_columns['shooting'] * numerical_columns['mentality_aggression']

  # Select relevant features
  selected_features = feature_importances[feature_importances['importance'] > 0.01]['feature'].tolist()
  X = numerical_columns[selected_features]

  # Combine numerical and categorical columns
  X = pd.concat([X, categorical_columns], axis=1)

  # Return the preprocessed data
  return X

# Load the saved ensemble model
ensemble_model = pkl.load(open('C:\\Users\\vera.anthonio\\Ensemble.pkl', 'rb'))

# Preprocess the test data using the same steps as the training data
X_test = preprocess_data('players_22-1.csv')

common_features = [feature for feature in X if feature in Xtest.columns]

# Select these columns from the test dataset
X_test = Xtest[common_features]

# Load the trained model (Ensemblemodel)
with open('C:\\Users\\vera.anthonio\\Ensemble.pkl',  'rb') as file:
    ensemble_model = pkl.load(file)

# Evaluate the model on the test data
y_pred = ensemble_model.predict(X_test)
test_mae = mean_absolute_error(actual_values[Ytest], y_pred)
test_mse = mean_squared_error(actual_values[Ytest], y_pred)
test_r2 = r2_score(actual_values[Ytest], y_pred)

print(f"Ensemble Model Test MAE: {test_mae}")
print(f"Ensemble Model Test MSE: {test_mse}")
print(f"Ensemble Model Test RÂ²: {test_r2}")